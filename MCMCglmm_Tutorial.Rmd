---
title: "Bayesian generalised mixed models with MCMCglmm"
author: "Ferran Sayol"
date: "20th of December 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

This is a short guide of running mixed models under a bayesian framework using the MCMCglmm R package. The course is structured in three parts: 1) See the basics of MCMCglmm models (run and check the output); 2) Adding random effects and modify the priors; 3) Correct for phylogenetic effects.

### Package Installation.

First we need to install the MCMCglmm package. We also install an additional packages to use phylogenetic trees (phytools).

```{r install_packages, results="hide", message=FALSE, warning=FALSE}
if(!require(MCMCglmm)) install.packages("MCMCglmm")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(phytools)) install.packages("phytools")
```

Next we load up the packages we just installed from the library and we are good to go.

```{r load_pakages, results="hide", message=FALSE, warning=FALSE}
library(MCMCglmm)
library(phytools)
library(ggplot2)
```

## PART 1: Introduction to MCMCglmm models

First we will load some data as an example. We will use data on morphological measurements and ecology of Pigeons&Doves (Columbidae).

```{r load_data, message=FALSE, warning=FALSE}
getwd() #Check the Working directory
#setwd() #Change the working directory.
dove <- read.table("DB_ColumbidaeData.txt",h=T)
```

This data file is based on a subset of the data used in an analysis on the relation between foraging behaviour and the evolution of morphological adaptations (Lapiedra et al. 2013)
\href{https://royalsocietypublishing.org/doi/full/10.1098/rspb.2012.2893}{Link to paper}. Note that from the original data we have generated N repeated measures for each species, adding random noise on each data point.

```{r show_data, message=FALSE, warning=FALSE}
head(dove)
```

Let's pretend our goal is to study the relation between morphology and ecology. For instance, if the tarsus length is related to foraging behaviour. But we will first start by exploring the relation between tarsus length and body size.

```{r scatterplot, message=FALSE, warning=FALSE}
ggplot(dove, aes(x=log(mass.g), y=log(tarsus.mm))) +
  geom_point(shape=21)
```

Now let's run a simple model with tarsus.mm as response of body size (mass.g).

When running an MCMCglmm, we need to specify some parameters of the mcmc chain: How many iterations we want to run the chain for (nitt), the burnin we want to discard at the start of the chain (burnin) and also how often we want to sample and store from the chain (`thin`). We discard a burnin as we don't want the starting point of the chain to over-influence our final estimates.

```{r setting formula, message=FALSE, warning=FALSE}
prior1 <- list(R=list(V = 1,nu = 0.002)) #We will see this later

mod1.1 <- MCMCglmm(log(tarsus.mm) ~ log(mass.g), 
                     data = dove, prior = prior1,verbose=F,
                     nitt = 100, thin=1, burnin = 1)
```

Before we even look at our model putput we can check if the model ran appropriately. We can do this by visually inspecting the chains. We can extract the full chains using `model$Sol` for the fixed effects and `model$VCV` for the variance terms. So `Sol[,1]` will give you the first fixed term, in this case the intercept, and `VCV[,1]` will give you the first random term, which is just the residual term here. As our model is an mcmc object when we use the plot function we get a trace plot.

```{r MCMCglmm_plot, message=FALSE, warning=FALSE, verbose = FALSE}
#plot the fist fixed term, the intercpet.
plot(mod1.1$Sol)
#plot the fist variance term, the residual error term.
plot(mod1.1$VCV)
```

On the right hand side of the plots is the posterior distributions for each of the terms. On the left side of these plots are the traces of the mcmc chain for each estimate. What we want to see in these trace plots has an aparent random pattern. That is a trace with no obvious trend that is bouncing around some stable point. 

Another thing we also want to check is the level of auto-correlation in the chain traces. We can do this using autocorr.diag() which gives the level of correlation along the chain between some lag sizes.

Let's see some diagnosis (Autocorrelation)

```{r autocorr, message=FALSE, warning=FALSE}
autocorr.diag(mod1.1$Sol) #Solutions (coeficients)
autocorr.diag(mod1.1$VCV) #Variance
```

Another way is to look at autocorrelation plots for each of the traces. For example, let's check the auto-correlation in the intercept chain using the `acf` function

```{r acf, message=FALSE, warning=FALSE, verbose = FALSE}
#acf plot for the first fixed estimate in our model (the intercept)
acf(mod1.1$Sol[,1],lag.max =100)
```

Ideally, we have to make sure that the autocorrelation is less than 0.1. The thinning is used to help reduce autocorrelation in our sample, how much you use often depends on how much autocorrelation you find and we can reduce autocorrelation by increasing the thining interval. As a result, we might have to increase the total number of iterations as well to have a sample of at least 1000. We can also set a Burn-in (normally 5-10% of samples) to get rid of the first samples that have not converged yet.

-----

* EXERCISE 1:
*Increase the thining interval and the number of iterations to make sure there is no autocorrelation and to have a sample of >1000.*

```{r Exercise1, message=FALSE, warning=FALSE}
prior1 <- list(R=list(V = 1,nu = 0.002)) #We will see this later

mod1.1 <- MCMCglmm(log(tarsus.mm) ~ log(mass.g), 
                     data = dove, prior = prior1,verbose=T,
                     nitt = 101000, thin=100, burnin = 1000)
```

-----

Now, let's explore the output of model.

```{r summary model, message=FALSE, warning=FALSE}
summary(mod1.1)
```

We can see the estimates for the fixed factor. Each parameter has a measure of the effect size under post.mean and a lower and higher 95% credible interval (CI).

Another way to directly look at the posterior means and confidence intervals for the factors is with the following commands.

Posterior mean of fixed factors:
```{r Posterior mode, message=FALSE, warning=FALSE}
posterior.mode(mod1.1$Sol)
```

Posterior mean of fixed factors:
```{r 95% Intervals, message=FALSE, warning=FALSE}
HPDinterval(mod1.1$Sol)
```

We also have the effective sample size (*eff.samp*) and the *pMCMC* which calculated as two times the probability that the estimate is either > or <  0, using which ever one is smaller. However, since our data has been mean centred and expressed in units of standard deviation we can simply look at what proportion of our posterior is on either side of zero. This mean centering and expression in of our data units of standard deviation hence allows us to use a cut off point like a p-value but without boiling down the whole distribution to one value.

We also have the *DIC* which is a Bayesian version of *AIC*. Like *AIC* it is a measure of the trade-off between the "fit" of the model and the number of parameters, with a lower number better.

Comming back to theinitial question, we want to see if foraging behavioir can explain tarsus length while accounting for body size. Let's do a plot first:

```{r scatterplot colors, message=FALSE, warning=FALSE}
ggplot(dove, aes(x=log(mass.g), y=log(tarsus.mm), color=foraging)) +
  geom_point(shape=19)
```

It looks like foraging behaviour can explain relative differences in tarsus length. But we need to test this:

-----

* EXERCISE 2:
*Run a new model (mod1.2) including also the foraging ecology together with body size as predictor and compare the DIC with mod1.1 / Which of the models is better?*

```{r Exercise2, message=FALSE, warning=FALSE}
prior1 <- list(R=list(V = 1,nu = 0.002)) #We will see this later

names(dove)
mod1.2 <- MCMCglmm(log(tarsus.mm) ~ log(mass.g)+foraging, 
                     data = dove, prior = prior1,verbose=F,
                     nitt = 101000, thin=100, burnin = 1000)
mod1.3 <- MCMCglmm(log(tarsus.mm) ~ log(mass.g)+foraging-1, 
                     data = dove, prior = prior1,verbose=F,
                     nitt = 101000, thin=100, burnin = 1000)

summary(mod1.2)
summary(mod1.3)


```

```{r scatterplot colors + model, message=FALSE, warning=FALSE}
ggplot(dove, aes(x=log(mass.g), y=log(tarsus.mm), color=foraging)) +
  geom_point(shape=19) + 
  geom_abline(intercept = posterior.mode(mod1.3$Sol)[2],slope = posterior.mode(mod1.3$Sol)[1],color="red")+
  geom_abline(intercept = posterior.mode(mod1.3$Sol)[3],slope = posterior.mode(mod1.3$Sol)[1],color="blue")

summary(mod1.1)

```


-----

### Model convergence

One last thing to check is that our MCMC chain has properly converged and that our estimate is not the result of some type of transitional behaviour. That is have our chains "found" the optimum or do we need to let them run longer before they settle around some estimate. To check this we will run a second model and see if it converges on the same estimates as our first model.

```{r model convergence, message=FALSE, warning=FALSE}
mod1.3 <- MCMCglmm(log(tarsus.mm) ~ log(mass.g)+foraging, 
                     data = dove, prior = prior1,verbose=F,
                     nitt = 101000, thin=100, burnin = 1000)
mod1.3b <- MCMCglmm(log(tarsus.mm) ~ log(mass.g)+foraging, 
                     data = dove, prior = prior1,verbose=F,
                     nitt = 101000, thin=100, burnin = 1000)
#They have reached the same solution?
plot(mcmc.list(mod1.3$Sol[,2], mod1.3b$Sol[,2]))
summary(mod1.3b)
```

## PART 2: Modify priors and add random factors

Since we are using a Bayesian approach we will need to set up the priors. In most cases we want to use a non-informative prior that doesn't influence the estimated posterior distribution. We are basically saying that we don't know anything about the expected values for our parameters. That is we have no prior information.

To give priors for MCMCglmm we need to make an object that is in a list format that includes terms B (fixed effects), R (residual terms) and G (random effects).

In our model we have 3 fixed terms B (1 intercept + 2 factors) and the residual term R.

```{r priors, message=FALSE, warning=FALSE}
prior2.1 <- list(R=list(V = 1,nu = 0.002))

mod2.1 <- MCMCglmm(log(tarsus.mm) ~ foraging+log(mass.g), 
                     data = dove, prior = prior2.1,verbose=F,family="gaussian",
                     nitt = 1100, thin=10, burnin = 100)
summary(mod2.1)
```

For fixed effects (B) the terms mu and V give the variance and mean of a normal distribution. Here we set mu as 0 and the variance as a large number to make these priors uninformative. Since we have three fixed terms (two intercepts and one slope) we can use the diag function to create a matrix to store a prior for each. Normally we don't need to set this as MCMCglmm will set non-informative priors automatically for fixed terms. Then, we can set the prior by only specifying the R term:

```{r simplified prior, message=FALSE, warning=FALSE}
prior2.1 <- list(R=list(V = 1,nu = 0.002))
```

For any of the variance terms (R or G) we need to make sure that the distribution is bounded at zero as the variance term needs to be positive. In MCMCglmm the variance is described the parameters *nu* and *V*. As again we don't have any prior information, we will use weakly informative prior values such as descripted as *V* = 1 and *nu* = 0.002. 

### Mixed models: Adding random factors to our MCMCglmm

Just to remember, mixed models are referred to models that contain both fixed and random factors.

Let's add a random term of measurement ("measureID" > Who took the measure) in the *dove* example. Like before, we need to set up the prior however we will let the model estimte the fixed effects this time. To add a random term we now add a *G* structure that acts just like the other random varience term and is defined using *nu* and *V*.

```{r Add random to prior mixed, message=FALSE, warning=FALSE}
prior2.2 <- list(G = list(G1 = list(nu=0.002, V=1),G2 = list(nu=0.002, V=1)),
              R = list(nu=0.002, V=1))
```

We can now include the random term in the model in the section `random= ~`.

Here, we will include "measureID" as a random effect, as we have repeated measures for each of the species.


```{r run mixed effects MCMCglmm, message=FALSE, warning=FALSE, verbose = FALSE}
table(dove$measureID)
table(dove$location)
names(dove)
mod2.2 <- MCMCglmm(log(tarsus.mm) ~ foraging+log(mass.g),
                     random= ~measureID+location,
                     data = dove, prior = prior2.2,verbose=F,
                     nitt = 1100, thin=10, burnin = 100)
summary(mod2.2)
```

-----

* EXECISE 3:
*Include also the geographical location ("location") as a random effect. Remember you will need to specify the prior for this new factor as well.*

-----


## PART 3: Phylogenetic effects and random variance.

As species are not independent of each other due to shared ancestry, we need to take this into account. MCMCglmm allows to include phylogenetic similarity as a random effect. For this, we only need a phylogenetic tree and a column in our data called 'animal' that corresponds to the phylogenetic tips of the tree.

We open the tree and plot it:

```{r Plot tree, message=FALSE, warning=FALSE, verbose = FALSE}
tree <- read.tree("ColumbidaeTree.tre")
plot(tree,cex=0.3)
```

Now, we add a column in our data with the tips of the tree. We already have a column "species", but MCMCglmm need a column names "animal" to associate with the tree.

```{r Add phylogenetic effects, message=FALSE, warning=FALSE, verbose = FALSE}
dove$animal <- dove$species
prior3.1 <- list(G = list(G1 = list(nu=0.002, V=1),G2 = list(nu=0.002, V=1)),
                 R = list(nu=0.002, V=1))

mod3.1 <- MCMCglmm(log(tarsus.mm) ~ 1+log(mass.g),
                     random= ~animal + measureID,
                     data = dove, prior = prior3.1,verbose=F,
                     pedigree=tree,
                     nitt = 11000, thin=10, burnin = 100)
summary(mod3.1)
```

We can see that different random factors explain different proportion of the variance. We can explore the effect of each random factor:

```{r Check variance, message=FALSE, warning=FALSE, verbose = FALSE}
posterior.mode(mod3.1$VCV)
```

However, it's more appropriate to report the intraclass correlation as the proportion of variance explained by each random factor in relative terms. This is done by dividing the variance of factor X by the sum of all varainces.

So the proportion of variance explained by the phylogeny is:

```{r Intraclass coefficient, message=FALSE, warning=FALSE, verbose = FALSE}
total.variance <- sum(posterior.mode(mod3.1$VCV))
IC.animal <- posterior.mode(mod3.1$VCV)[1]/total.variance #animal variance
IC.animal
```

IC.animal is expressed in relation to the total variance (1). If we want the % we can do:

```{r Animal %, message=FALSE, warning=FALSE, verbose = FALSE}
round(IC.animal*100,2) #round to 2 decimals and multiple by 100
```

This is a useful propierty of the MCMCglmm models, as we can see which is the phylogenetic effect (or heterability) of traits or can calculate the repeatability of measurements.

----

*EXERCISE 4: Compare the proportion of variance
*Include the "measureID" and "location" as a random factor in a new model. Which factors explains more proportion of the variance?*

----

For more information see: 

https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf

